{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:52.921008Z",
     "start_time": "2024-10-31T13:39:52.890223Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration\n",
    ")\n",
    "\n",
    "from transformers import logging as transformers_logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "transformers_logging.set_verbosity_error()\n",
    "import json\n",
    "import time\n",
    "from functools import wraps\n",
    "import sentencepiece\n",
    "\n",
    "# Silencing TqdmWarning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:52.964574Z",
     "start_time": "2024-10-31T13:39:52.947941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words_to_keep = [\"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"with\", \"how\", \"before\", \"after\",\"same\"]\n",
    "stop_words = set([s for s in stopwords.words('english') if s not in stop_words_to_keep])"
   ],
   "id": "9ce915e61e55399c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sandr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:52.986534Z",
     "start_time": "2024-10-31T13:39:52.981751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def measure_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Record the start time\n",
    "        result = func(*args, **kwargs)  # Call the function\n",
    "        end_time = time.time()  # Record the end time\n",
    "        elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "        print(f\"Execution time for {func.__name__}: {elapsed_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "id": "987d727174eaa4ee",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:53.014686Z",
     "start_time": "2024-10-31T13:39:53.004380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NERParser:\n",
    "    def __init__(self, model_name=\"dslim/bert-base-NER\", lowercase=False):\n",
    "        self.lowercase = lowercase\n",
    "        self.device = self.get_device()\n",
    "\n",
    "        self.nlp_pipeline = pipeline(\n",
    "            \"ner\", \n",
    "            model=AutoModelForTokenClassification.from_pretrained(model_name),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model_name, do_lower_case=lowercase),\n",
    "            device=self.device, \n",
    "            aggregation_strategy=\"simple\"\n",
    "        )\n",
    "\n",
    "    def get_device(self):\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    def parse_ner_results(self, ner_results):\n",
    "        per_entities = [e['word'] for e in ner_results if e['entity_group'] == 'PER']\n",
    "        misc_entities = [e['word'] for e in ner_results if e['entity_group'] == 'MISC']\n",
    "        return per_entities, misc_entities\n",
    "\n",
    "    def process_query(self, query):\n",
    "        if self.lowercase:\n",
    "            query = query.lower()\n",
    "        return self.parse_ner_results(self.nlp_pipeline(query))\n",
    "\n"
   ],
   "id": "a15b5f4fab282289",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:53.046807Z",
     "start_time": "2024-10-31T13:39:53.021696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataBase:\n",
    "    \"\"\"Handles context data extraction for people and movies from a database with fuzzy matching support.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.db = pd.read_pickle(os.path.join(os.getcwd(), \"exports/extended_graph_triples.pkl\"))\n",
    "        \n",
    "        self.db['subject_id'] = self.db['subject_id'].astype(str).str.strip()\n",
    "        self.db['predicate_label'] = self.db['predicate_label'].astype(str).str.strip()\n",
    "        self.db['object_label'] = self.db['object_label'].astype(str).str.strip()\n",
    "        \n",
    "        self.db_pivot = self.db.pivot_table(\n",
    "                index='subject_id',\n",
    "                columns='predicate_label',\n",
    "                values='object_label',\n",
    "                aggfunc=lambda x: ' | '.join(x.astype(str))\n",
    "            )\n",
    "        \n",
    "        with open('exports/movie_db.json') as f:\n",
    "            self.movie_data = json.load(f)\n",
    "            self.movie_ids = set(self.movie_data.keys())\n",
    "        self.movie_db = pd.DataFrame(list(self.movie_data.items()), columns=[\"entity_id\", \"entity_label\"])\n",
    "        \n",
    "        with open('exports/people_db.json') as f:\n",
    "            self.people_data = json.load(f)\n",
    "            self.people_ids = set(self.people_data.keys())\n",
    "        self.people_db = pd.DataFrame(list(self.people_data.items()), columns=[\"entity_id\", \"entity_label\"])\n",
    "        \n",
    "        self.entities = {**self.movie_data, **self.people_data}\n",
    "        \n",
    "        self.movie_names = self.movie_db[\"entity_label\"].tolist()\n",
    "        self.people_names = self.people_db[\"entity_label\"].tolist()\n",
    "        \n",
    "        self.entity_list = self.movie_names + self.people_names\n",
    "       \n",
    "        self.people_movie_mapping = {}\n",
    "        self.movie_people_mapping = {}\n",
    "        \n",
    "        self.map_people_movies()\n",
    "        \n",
    "        self.movie_recommender_db = self.filter_relevant_movies()\n",
    "        \n",
    "    def map_people_movies(self):\n",
    "        id_triples = pd.read_pickle(os.path.join(\"exports/df_new_triples_only_ids.pkl\"))\n",
    "        \n",
    "        id_triples['subject_id'] = id_triples['subject_id'].astype(str).str.strip()\n",
    "        id_triples['object_id'] = id_triples['object_id'].astype(str).str.strip()\n",
    "        \n",
    "        for _, row in id_triples.iterrows():\n",
    "            subject_id = row['subject_id']\n",
    "            object_id = row['object_id']\n",
    "            \n",
    "            if subject_id in self.people_ids and object_id in self.movie_ids:\n",
    "                if subject_id not in self.people_movie_mapping:\n",
    "                    self.people_movie_mapping[subject_id] = []\n",
    "                self.people_movie_mapping[subject_id].append(object_id)\n",
    "                \n",
    "                if object_id not in self.movie_people_mapping:\n",
    "                    self.movie_people_mapping[object_id] = []\n",
    "                self.movie_people_mapping[object_id].append(subject_id)\n",
    "        \n",
    "        for person, movies in self.people_movie_mapping.items():\n",
    "            self.people_movie_mapping[person] = list(set(movies))\n",
    "        \n",
    "        for movie, people in self.movie_people_mapping.items():\n",
    "            self.movie_people_mapping[movie] = list(set(people))\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def normalize_string(s):\n",
    "        \"\"\"Normalizes strings by removing non-ASCII characters, punctuation, and redundant spaces.\"\"\"\n",
    "        return ' '.join(re.sub(r'[^\\w\\s]', '', unicodedata.normalize('NFKD', s.lower())\n",
    "                               .encode('ascii', 'ignore').decode('utf-8')).split())\n",
    "\n",
    "    def fetch(self, entity_list, search_column):\n",
    "        \"\"\"Fetches relevant rows from the database where `search_column` matches values in `entity_list`.\"\"\"\n",
    "        relevant = self.db[self.db[search_column].isin(entity_list)].dropna(axis=1)\n",
    "        \n",
    "        if relevant.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return relevant.pivot_table(\n",
    "            index='subject_id',\n",
    "            columns='predicate_label',\n",
    "            values='object_label',\n",
    "            aggfunc=lambda x: ' | '.join(x.astype(str))\n",
    "        ).reset_index()\n",
    "    \n",
    "    def filter_relevant_movies(self):\n",
    "\n",
    "        clean_db = self.db[self.db[\"subject_id\"].isin(self.movie_data.keys())]\n",
    "        relevant_cols = [\n",
    "            # \"author\", # only 99 movies hav an author\n",
    "            # \"cast member\",\n",
    "            \"director\", \n",
    "            \"performer\",\n",
    "            \"genre\",\n",
    "            # \"narrative motif\", # only 43 movies have a narrative motif\n",
    "            \"screenwriter\",\n",
    "            \"subject_id\",\n",
    "            \"node label\" # Required for processing\n",
    "        ]\n",
    "        \n",
    "        pv_db = clean_db.pivot_table(\n",
    "                index='subject_id',\n",
    "                columns='predicate_label',\n",
    "                values='object_label',\n",
    "                aggfunc=lambda x: ' | '.join(x.astype(str))\n",
    "            )\n",
    "        \n",
    "        pv_db = pv_db[[col for col in relevant_cols if col in pv_db.columns]]\n",
    "\n",
    "        return pv_db.reset_index()"
   ],
   "id": "c9bf6d33ae34d781",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:53.065779Z",
     "start_time": "2024-10-31T13:39:53.057223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QueryEmbedderContextualized:\n",
    "    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "        \"\"\"Initializes the QueryEmbedder with a SentenceTransformer model and device setup.\"\"\"\n",
    "        self.device = self.get_device()\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "        self.cache = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_device():\n",
    "        \"\"\"Determines the available hardware device (MPS, CUDA, or CPU).\"\"\"\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    def embed_phrase(self, phrases):\n",
    "        \"\"\"\n",
    "        Generates embeddings for given phrases using SentenceTransformer, with caching.\n",
    "\n",
    "        Args:\n",
    "            phrases (str or List[str]): Input phrase(s) to embed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Embedding vector(s) for the input phrase(s).\n",
    "        \"\"\"\n",
    "        if isinstance(phrases, str):\n",
    "            phrases = [phrases]\n",
    "        elif not isinstance(phrases, list):\n",
    "            raise TypeError(\"Input must be a string or a list of strings.\")\n",
    "        \n",
    "        phrases_to_compute = [p for p in phrases if p not in self.cache]\n",
    "        cached_embeddings = [self.cache[p] for p in phrases if p in self.cache]\n",
    "\n",
    "        if phrases_to_compute:\n",
    "            new_embeddings = self.model.encode(\n",
    "                phrases_to_compute, \n",
    "                show_progress_bar=False, \n",
    "                convert_to_numpy=True, \n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            for phrase, emb in zip(phrases_to_compute, new_embeddings):\n",
    "                self.cache[phrase] = emb\n",
    "            cached_embeddings.extend(new_embeddings)\n",
    "        \n",
    "        return cached_embeddings[0] if len(cached_embeddings) == 1 else np.array(cached_embeddings)\n"
   ],
   "id": "b6108d2c6628d7db",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:53.079407Z",
     "start_time": "2024-10-31T13:39:53.072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QuestionAnsweringAgent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.qa_model = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\", top_k=1)\n",
    "    \n",
    "    def query(self, query, context_df):\n",
    "        \n",
    "        top_columns = context_df.columns\n",
    "        \n",
    "        context = \"\"\n",
    "        for index, row in context_df.iterrows():\n",
    "            node_label = row.get(\"node label\", \"\")\n",
    "            \n",
    "            row_context = f\"This text is about \\\"{node_label}\\\":\\n\"\n",
    "            \n",
    "            for col in context_df[top_columns].columns:\n",
    "                if col == \"node label\":\n",
    "                    continue\n",
    "                \n",
    "                values = row[col]\n",
    "                values_lst = str(values).split(\",\")\n",
    "                \n",
    "                if len(values_lst) > 5:\n",
    "                    row_context += f\"{col}: {', '.join(values_lst[:5])}\"\n",
    "                else:\n",
    "                    row_context += f\"{col}: {', '.join(values_lst)}\"\n",
    "\n",
    "            context += row_context + \"\\n\\n\"\n",
    "        \n",
    "        output = self.qa_model(question=query, context=context)\n",
    "        \n",
    "        answer_str = str()\n",
    "        if isinstance(output, list) and output:\n",
    "            answer_str = \", \".join([result['answer'] for result in output])\n",
    "            \n",
    "        elif isinstance(output, dict):\n",
    "            answer_str = output['answer']\n",
    "        \n",
    "        if not answer_str:\n",
    "            answer_str = \"No answer found.\"\n",
    "        \n",
    "        return answer_str"
   ],
   "id": "9e2df2f00abd15da",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:53.093634Z",
     "start_time": "2024-10-31T13:39:53.087576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConversationAgent:\n",
    "    def __init__(self, model_name=\"google/flan-t5-large\", max_length=150):\n",
    "        self.device = self.get_device()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    @staticmethod\n",
    "    def get_device():\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        \"\"\"\n",
    "        Generates a response based on the given prompt.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_length=self.max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response.strip()\n"
   ],
   "id": "adbe62e7f66579e2",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:53.112234Z",
     "start_time": "2024-10-31T13:39:53.099482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import rdflib\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "class GraphEmbeddings:\n",
    "    \n",
    "    def __init__(self, graph):\n",
    "        \n",
    "        self.RDFS = rdflib.namespace.RDFS\n",
    "        self.WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
    "     \n",
    "        self.entity_emb = np.load('exports/entity_embeds.npy')\n",
    "        self.relation_emb = np.load('exports/relation_embeds.npy')\n",
    "        \n",
    "        with open('exports/entity_ids.del', 'r') as ifile:\n",
    "            self.ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "            self.id2ent = {v: k for k, v in self.ent2id.items()}\n",
    "            \n",
    "        with open('exports/relation_ids.del', 'r') as ifile:\n",
    "            self.rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "        \n",
    "        with open(\"exports/predicate_db.json\", encoding=\"utf-8\") as f:\n",
    "            self.predicates_db = json.load(f)\n",
    "        \n",
    "        self.ent2lbl = {rdflib.term.URIRef(row.subject_id): row.subject_label for index, row in graph.iterrows()}\n",
    "\n",
    "    def answer_query_embedding(self, context, top_columns):\n",
    "        STD_ERROR = \"The embeddings did not provide a valid answer.\"\n",
    "        try:\n",
    "            context_id = context.subject_id.values[0]\n",
    "            if not context_id:\n",
    "                print(\"No context ID found.\")\n",
    "                return STD_ERROR\n",
    "\n",
    "            e_id = self.ent2id.get(rdflib.term.URIRef(context_id))\n",
    "            if e_id is None:\n",
    "                return \"2\"\n",
    "\n",
    "            head = self.entity_emb[e_id]\n",
    "            wiki_predicate_id = \"\"\n",
    "\n",
    "            column_mapping = {\n",
    "                \"movie cast\": \"cast member\",\n",
    "                \"acted in\": \"notable work\",\n",
    "                \"played in\": \"notable work\",\n",
    "                \"appeared in\": \"notable work\",\n",
    "                \"actors\": \"cast member\",\n",
    "                \"players\": \"cast member\"\n",
    "            }\n",
    "\n",
    "            for col in top_columns:\n",
    "                \n",
    "                col = column_mapping.get(col, col)\n",
    "                \n",
    "                if col == \"node label\":\n",
    "                    continue\n",
    "                    \n",
    "                if not context[col].values[0]:\n",
    "                    continue\n",
    "\n",
    "                if col in self.predicates_db:\n",
    "                    wiki_id = rdflib.term.URIRef(self.predicates_db[col])\n",
    "                    if wiki_id in self.rel2id:\n",
    "                        wiki_predicate_id = self.predicates_db[col]\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Predicate {col} not found in rel2id.\")\n",
    "                else:\n",
    "                    print(f\"Predicate {col} not found in predicate_db.\")\n",
    "\n",
    "            if not wiki_predicate_id:\n",
    "                print(\"No valid predicate found.\")\n",
    "                return STD_ERROR\n",
    "\n",
    "            r_id = self.rel2id[rdflib.term.URIRef(wiki_predicate_id)]\n",
    "            pred = self.relation_emb[r_id]\n",
    "\n",
    "            lhs = head + pred\n",
    "            dist = pairwise_distances(lhs.reshape(1, -1), self.entity_emb).reshape(-1)\n",
    "            most_likely = dist.argsort()\n",
    "\n",
    "            num_results = min(3, len(most_likely))\n",
    "            results_lst = [\n",
    "                (self.id2ent[idx][len(self.WD):], self.ent2lbl.get(self.id2ent[idx], \"\"), dist[idx], rank + 1)\n",
    "                for rank, idx in enumerate(most_likely[:num_results])\n",
    "            ]\n",
    "            results_df = pd.DataFrame(results_lst, columns=('Entity', 'Label', 'Score', 'Rank'))\n",
    "            \n",
    "            if results_df.empty:\n",
    "                print(\"No results in embeddings found.\")\n",
    "                return STD_ERROR\n",
    "\n",
    "            return f\"Top answers from embeddings: {', '.join(results_df.Label.values)}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during query answering: {e}\")\n",
    "            return STD_ERROR\n",
    "        "
   ],
   "id": "7c4c6264fcca0ba0",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:53.128772Z",
     "start_time": "2024-10-31T13:39:53.118877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0 \n",
    "    \n",
    "    return np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)\n",
    "\n",
    "\n",
    "def rescale_probabilities(similarities):\n",
    "    \"\"\"\n",
    "    Rescales the similarity scores so that they sum to 1, turning them into a probability distribution.\n",
    "    \n",
    "    Args:\n",
    "        similarities (List[float]): List of similarity scores.\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: Rescaled probabilities.\n",
    "    \"\"\"\n",
    "    similarity_sum = sum(similarities)\n",
    "    if similarity_sum == 0:\n",
    "        return [0] * len(similarities)  # Avoid division by zero\n",
    "    \n",
    "    return [sim / similarity_sum for sim in similarities]\n",
    "\n",
    "def find_closest_columns(query_embeddings, column_embeddings, high_threshold=0.4, top_n=10, rescaled_threshold=0.11):\n",
    "    \"\"\"\n",
    "    Returns columns based on cosine similarity with a two-tiered strategy and rescaled probabilities.\n",
    "    - If a column has similarity above 'high_threshold', return that column immediately.\n",
    "    - Otherwise, return all columns with a similarity greater than 'low_threshold'.\n",
    "    - Rescale the top N column similarities into probabilities and return columns with a rescaled probability greater than rescaled_threshold.\n",
    "    \n",
    "    Args:\n",
    "        query_embeddings (List[np.ndarray]): Embeddings for query words.\n",
    "        column_embeddings (Dict[str, np.ndarray]): Precomputed embeddings for columns.\n",
    "        low_threshold (float): Minimum similarity threshold (default: 0.27).\n",
    "        high_threshold (float): Confidence threshold to return immediately (default: 0.35).\n",
    "        top_n (int): Number of top columns to consider for rescaling (default: 10).\n",
    "        rescaled_threshold (float): Minimum rescaled probability threshold (default: 0.1).\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: The selected column names.\n",
    "    \"\"\"\n",
    "    column_similarities = {}\n",
    "\n",
    "    for col, col_vec in column_embeddings.items():\n",
    "        similarities = [cosine_sim(col_vec, q_vec) for q_vec in query_embeddings if np.linalg.norm(q_vec) > 0]\n",
    "        column_similarities[col] = np.mean(similarities) if similarities else -1\n",
    "\n",
    "    sorted_columns = sorted(column_similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_columns = sorted_columns[:top_n]\n",
    "    \n",
    "    column_names, similarities = zip(*top_columns)\n",
    "    \n",
    "    rescaled_probs = rescale_probabilities(similarities)\n",
    "    \n",
    "    selected_columns = []\n",
    "    \n",
    "    for col, sim in zip(column_names, similarities):\n",
    "        if sim >= high_threshold:\n",
    "            # print(f\"High confidence match found: {col} with similarity {sim: .4f}\")\n",
    "            return [col]\n",
    "    \n",
    "    for col, rescaled_prob in zip(column_names, rescaled_probs):\n",
    "        if rescaled_prob >= rescaled_threshold:\n",
    "            # print(f\"Column {col} has similarity {rescaled_prob: .4f}\")\n",
    "            selected_columns.append(col)\n",
    "    \n",
    "    return selected_columns"
   ],
   "id": "9b901d7c50e94cbf",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:53.142965Z",
     "start_time": "2024-10-31T13:39:53.136400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_query(query, node_label):\n",
    "                \n",
    "    if not len(query):\n",
    "        return []\n",
    "    \n",
    "    relevant = []\n",
    "    for word in query.replace(\". \", \" \").lower().split(\" \"):\n",
    "        cleaned_word = re.sub(r'[^A-Za-z]', '', word)\n",
    "        if cleaned_word in stop_words or cleaned_word in node_label.lower().replace(\" \", \"\") or cleaned_word == \"\":\n",
    "            continue\n",
    "        \n",
    "        relevant.append(cleaned_word)\n",
    "        \n",
    "    return \" \".join(relevant)"
   ],
   "id": "6f3a2fd9e6e77d97",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:34:22.877090Z",
     "start_time": "2024-10-31T14:34:22.871046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fuzzy_match(query_str, comparison_list, threshold=30, prioritize_exact=True):\n",
    "    if not comparison_list or not query_str:\n",
    "        return []\n",
    "    \n",
    "    matches = process.extract(query_str, comparison_list, scorer=fuzz.partial_ratio, limit=50)\n",
    "        \n",
    "    id_name_score = []\n",
    "    \n",
    "    if prioritize_exact and query_str in comparison_list:\n",
    "        matched_id = next(key for key, value in db.entities.items() if value == query_str)\n",
    "        id_name_score.append((matched_id, query_str, 100))\n",
    "    \n",
    "    for match in matches:\n",
    "        name = match[0]\n",
    "        score = match[1]\n",
    "        matched_id = next(key for key, value in db.entities.items() if value == name)\n",
    "        \n",
    "        length_diff = abs(len(name) - len(query_str)) / len(query_str)\n",
    "        adjusted_score = score * (1 - length_diff)\n",
    "\n",
    "        if db.entities[matched_id] in query_str:\n",
    "            adjusted_score = 100\n",
    "        print(db.entities[matched_id], adjusted_score)\n",
    "        \n",
    "        id_name_score.append((matched_id, name, adjusted_score))\n",
    "    \n",
    "    return [id for id, _, score in id_name_score if score >= threshold]"
   ],
   "id": "d2eb95481eacac33",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:39:53.177432Z",
     "start_time": "2024-10-31T13:39:53.170951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_matches(df, normalized_query, top_n=2):\n",
    "    concatenated_rows = df.apply(lambda row: ' '.join(row.astype(str)), axis=1).tolist()\n",
    "    \n",
    "    exact_matches = [i for i, row in enumerate(concatenated_rows) if normalized_query == row]\n",
    "    \n",
    "    if len(exact_matches) < top_n:\n",
    "        remaining_slots = top_n - len(exact_matches)\n",
    "        fuzzy_matches = process.extract(normalized_query, concatenated_rows, scorer=fuzz.partial_ratio, limit=remaining_slots)\n",
    "        fuzzy_indices = [match[2] for match in fuzzy_matches]\n",
    "    else:\n",
    "        fuzzy_indices = []\n",
    "    \n",
    "    top_indices = exact_matches + fuzzy_indices\n",
    "    \n",
    "    return df.iloc[top_indices]"
   ],
   "id": "49ae6c8781d022d6",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:42:55.672298Z",
     "start_time": "2024-10-31T13:39:53.187880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = DataBase()\n",
    "\n",
    "ner_parser = NERParser(lowercase=False)\n",
    "\n",
    "qe = QueryEmbedderContextualized()\n",
    "\n",
    "qa = QuestionAnsweringAgent()\n",
    "\n",
    "ca = ConversationAgent(model_name=\"google/flan-t5-xl\") #### Could take some time, approx. 10 GB storage :)\n",
    "\n",
    "ge = GraphEmbeddings(db.db)\n"
   ],
   "id": "a9288d5aec4110c2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:34:26.554765Z",
     "start_time": "2024-10-31T14:34:26.541057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import heapq\n",
    "\n",
    "@measure_time\n",
    "def answer_query(query, correct_answer=\"\"):\n",
    "    normalized_query = db.normalize_string(query)\n",
    "    \n",
    "    # NER --------------------------\n",
    "    ner_person, ner_movies = ner_parser.process_query(normalized_query)\n",
    "    ner_matches = ner_person + ner_movies\n",
    "    \n",
    "    is_domain_specific = bool(ner_matches)\n",
    "    # if not is_domain_specific:\n",
    "    #     small_talk = ca.generate_response(query)\n",
    "    #     print(f\"Smalltalk: {small_talk}\")\n",
    "    #     return\n",
    "    \n",
    "    print(f\"NER_Person: {ner_person}\")\n",
    "    print(f\"NER_Movies: {ner_movies}\")\n",
    "    \n",
    "    ner_people_ids = []\n",
    "    for ner_p in ner_person:\n",
    "        ner_p = db.normalize_string(ner_p)\n",
    "        if ner_p in db.people_names:\n",
    "            ner_people_ids = [key for key, value in db.people_data.items() if value == ner_p]\n",
    "        if not ner_people_ids:\n",
    "            ner_people_ids = [key for key, value in db.people_data.items() if ner_p in value]\n",
    "        if not ner_people_ids:\n",
    "            ner_people_ids = fuzzy_match(\" \".join(ner_person), db.people_names, threshold=75)\n",
    "    \n",
    "    ner_movie_ids = []     \n",
    "    for ner_m in ner_movies:\n",
    "        ner_m = db.normalize_string(ner_m)\n",
    "        if ner_m in db.movie_names:\n",
    "            ner_people_ids = [key for key, value in db.movie_data.items() if value == ner_m]\n",
    "        if not ner_people_ids:\n",
    "            ner_people_ids = [key for key, value in db.movie_data.items() if ner_m in value]\n",
    "        if not ner_people_ids:\n",
    "            ner_people_ids = fuzzy_match(\" \".join(ner_movies), db.movie_names, threshold=75)\n",
    "\n",
    "    ner_ids = ner_movie_ids + ner_people_ids\n",
    "    context = db.fetch(ner_ids, \"subject_id\")\n",
    "    # Context NER --------------------------\n",
    "    # Context Fuzzy --------------------------\n",
    "\n",
    "    if context.empty:\n",
    "        print(\"NER failed, proceeding with fuzzy matching.\")\n",
    "        fuzzy_person_matches = fuzzy_match(normalized_query, db.people_names, threshold=30)\n",
    "        fuzzy_movie_matches = fuzzy_match(normalized_query, db.movie_names, threshold=30)\n",
    "    \n",
    "        fuzzy_movie_context = db.fetch(fuzzy_movie_matches, \"subject_id\") \n",
    "        fuzzy_person_context = db.fetch(fuzzy_person_matches, \"subject_id\")\n",
    "        \n",
    "        if fuzzy_person_context.empty and fuzzy_movie_context.empty:\n",
    "            small_talk = ca.generate_response(query)\n",
    "            print(f\"Smalltalk: {small_talk}\")\n",
    "            return\n",
    "        \n",
    "        context = pd.concat([fuzzy_movie_context, fuzzy_person_context])\n",
    "    \n",
    "    \n",
    "    context = get_top_matches(context, normalized_query, top_n=1)    \n",
    "\n",
    "    \n",
    "    node_label = \"\"\n",
    "    if not context.empty and \"node label\" in context.columns and not context[\"node label\"].isna().values[0]:\n",
    "        node_label = context[\"node label\"].values[0]  \n",
    "        \n",
    "    entity_id = \"\"\n",
    "    if not context.empty and \"subject_id\" in context.columns and not context[\"subject_id\"].isna().values[0]:\n",
    "        entity_id = context[\"subject_id\"].values[0]\n",
    "\n",
    "\n",
    "    if \"CURRENT MODE\" == \"RECOMMENDER\":\n",
    "        node_label = db.normalize_string(node_label)\n",
    "        if node_label in db.people_names:\n",
    "            movie_ids = db.people_movie_mapping[entity_id]\n",
    "            context = db.fetch(movie_ids, \"subject_id\")\n",
    "            subject_labels = context[\"node label\"].tolist()\n",
    "            print(\", \".join(subject_labels))\n",
    "            return\n",
    "            \n",
    "        context.dropna(axis=1, inplace=True)\n",
    "        columns = [col for col in context.columns if col in db.movie_recommender_db.columns]\n",
    "        red_db = db.movie_recommender_db[columns]\n",
    "        context = context[columns]\n",
    "        \n",
    "        # drop the identified row already in the context\n",
    "        subject_id_to_remove = context[\"subject_id\"].values[0]\n",
    "        red_db = red_db[red_db[\"subject_id\"] != subject_id_to_remove]\n",
    "        \n",
    "        red_db.dropna(thresh=len(red_db.columns) - 0, inplace=True)\n",
    "        red_db.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        def calculate_similarity(i, row):\n",
    "            similarities = []\n",
    "            for col in context.columns:\n",
    "                if pd.isna(row[col]):\n",
    "                    continue\n",
    "                    \n",
    "                if col in [\"node label\", \"subject_id\"]:\n",
    "                    continue\n",
    "                \n",
    "                similarity = fuzz.ratio(row[col], context[col].iloc[0]) / 100.0\n",
    "                similarities.append(similarity)\n",
    "    \n",
    "            return (i, np.mean(similarities) if similarities else 0)\n",
    "        \n",
    "        top_scores = []\n",
    "        for i, row in red_db.iterrows():\n",
    "            index, score = calculate_similarity(i, row)\n",
    "            \n",
    "            if len(top_scores) < 3:\n",
    "                heapq.heappush(top_scores, (score, index))\n",
    "            else:\n",
    "                # Maintain top 3 scores\n",
    "                heapq.heappushpop(top_scores, (score, index))\n",
    "    \n",
    "        # Extract indices from top 3 scores\n",
    "        top_indices = [index for score, index in top_scores]\n",
    "        top_rows = red_db.iloc[top_indices]\n",
    "        subject_labels = top_rows['node label'].tolist() \n",
    "        print(\", \".join(subject_labels))\n",
    "\n",
    "    elements_to_remove = [\"image\", \"color\", \"sport\"]\n",
    "    context = context.drop(columns=elements_to_remove, errors='ignore')\n",
    "    \n",
    "    # Initial context for embeddings\n",
    "    initial_context = context.copy()\n",
    "    \n",
    "    # EXPERIMENTAL - rename columns\n",
    "    columns_to_rename = {\n",
    "        \"cast member\":\"movie cast\",\n",
    "        \"notable work\": \"acted in\"\n",
    "    }\n",
    "    \n",
    "    columns_to_rename = {k: v for k, v in columns_to_rename.items() if k in context.columns}\n",
    "    context = context.rename(columns=columns_to_rename)\n",
    "    \n",
    "    columns_to_duplicate = [(\"acted in\", \"played in\"),\n",
    "                            (\"acted in\", \"appeared in\"),\n",
    "                            (\"movie cast\", \"actors\"),\n",
    "                            (\"movie cast\", \"players\")]\n",
    "    \n",
    "    for col_to_duplicate, col in columns_to_duplicate: \n",
    "        try:\n",
    "            context[col] = context[col_to_duplicate].copy()\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    context.dropna(axis=1, inplace=True)\n",
    "    \n",
    "    query_filtered = filter_query(query, node_label)\n",
    "        \n",
    "    column_embeddings = {col: qe.embed_phrase(col) for col in context.columns}\n",
    "    query_embeddings = [qe.embed_phrase(word) for word in query_filtered.split()]  \n",
    "    top_columns_embeddings = find_closest_columns(query_embeddings, column_embeddings)\n",
    "        \n",
    "    # EXPERIMENTAL - always keep columns\n",
    "    col_always_keep = [\"node label\"]\n",
    "    \n",
    "    combined_columns = set(top_columns_embeddings + col_always_keep)\n",
    "    top_columns = [col for col in combined_columns if col in context.columns]\n",
    "    filtered_context_df = context[top_columns]\n",
    "\n",
    "    answer = qa.query(query, filtered_context_df)\n",
    "    formatted_answer = ca.generate_response(f\"Format the answer to the question into a sentence. If you think the answer is completely off, overrule with your own knowledge.Question: {query}\\nAnswer: {answer}\")\n",
    "        \n",
    "    print(ge.answer_query_embedding(initial_context, top_columns))\n",
    "    print(formatted_answer)\n"
   ],
   "id": "758512dcaa9b7d3d",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:34:33.781656Z",
     "start_time": "2024-10-31T14:34:27.204186Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"In Which movies did Tom Holland have a role?\")",
   "id": "a2546df235e50a6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER_Person: []\n",
      "NER_Movies: []\n",
      "NER failed, proceeding with fuzzy matching.\n",
      "tom holland 100\n",
      "tom holland 100\n",
      "tom hollander 25.58139534883721\n",
      "todd holland 23.255813953488378\n",
      "allan havey 20.930232558139533\n",
      "sam rolfe 16.74418604651163\n",
      "vera briole 20.46511627906977\n",
      "ian white 16.74418604651163\n",
      "tom nolan 16.279069767441865\n",
      "tom villa 16.279069767441865\n",
      "david tom 16.279069767441865\n",
      "alan hale 16.279069767441865\n",
      "alan hale 16.279069767441865\n",
      "al cole 12.522361359570658\n",
      "ja rule 12.522361359570658\n",
      "lana 6.976744186046516\n",
      "tom hillmann 20.93023255813954\n",
      "tom wolf 13.953488372093023\n",
      "mary holland 20.93023255813954\n",
      "tom choi 13.953488372093023\n",
      "tom volf 13.953488372093023\n",
      "matt holland 20.93023255813954\n",
      "matt holland 20.93023255813954\n",
      "john holland 20.93023255813954\n",
      "dave 6.976744186046516\n",
      "tom hale 13.953488372093023\n",
      "tom held 13.953488372093023\n",
      "arno 6.976744186046516\n",
      "ovid 6.976744186046516\n",
      "zola 6.976744186046516\n",
      "eva rose 13.953488372093023\n",
      "tim holt 13.953488372093023\n",
      "tim hill 13.953488372093023\n",
      "tom bell 13.953488372093023\n",
      "tom rolf 13.953488372093023\n",
      "ian whitcomb 20.56303549571604\n",
      "tina hirsch 18.849449204406366\n",
      "david hollander 25.58139534883721\n",
      "tore vollan 18.6046511627907\n",
      "alan chavez 18.6046511627907\n",
      "kim howland 18.6046511627907\n",
      "tomas hodan 18.6046511627907\n",
      "tom villard 18.6046511627907\n",
      "roy lee 11.839323467230441\n",
      "tom wayland 18.6046511627907\n",
      "ron lea 11.839323467230441\n",
      "ping wu 11.839323467230441\n",
      "dj wich 11.627906976744184\n",
      "oh land 11.627906976744184\n",
      "tom lay 11.627906976744184\n",
      "id 100\n",
      "id 100\n",
      "land ho 13.953488372093021\n",
      "sole 7.973421926910304\n",
      "vibes 9.302325581395348\n",
      "ing 5.58139534883721\n",
      "lie 5.58139534883721\n",
      "holly 9.302325581395348\n",
      "epic movie 18.604651162790695\n",
      "haven 9.302325581395348\n",
      "hitch 9.302325581395348\n",
      "carol 9.302325581395348\n",
      "havre 9.302325581395348\n",
      "we have a pope 25.044722719141326\n",
      "win win 12.522361359570658\n",
      "vips 6.976744186046516\n",
      "vlad 6.976744186046516\n",
      "rope 6.976744186046516\n",
      "ichi 6.976744186046516\n",
      "vile 6.976744186046516\n",
      "id a 6.976744186046516\n",
      "move 6.976744186046516\n",
      "hick 6.976744186046516\n",
      "howl 6.976744186046516\n",
      "tom horn 13.953488372093023\n",
      "vice 6.976744186046516\n",
      "rose 6.976744186046516\n",
      "hell 6.976744186046516\n",
      "love 6.976744186046516\n",
      "chic 6.976744186046516\n",
      "love 6.976744186046516\n",
      "dik trom 13.953488372093023\n",
      "olga 6.976744186046516\n",
      "nola 6.976744186046516\n",
      "golem 8.720930232558139\n",
      "hell 6.976744186046516\n",
      "dhol 6.976744186046516\n",
      "dave 6.976744186046516\n",
      "hell 6.976744186046516\n",
      "dtox 6.976744186046516\n",
      "rose 6.976744186046516\n",
      "lola 6.976744186046516\n",
      "vice 6.976744186046516\n",
      "love 6.976744186046516\n",
      "hell 6.976744186046516\n",
      "vice 6.976744186046516\n",
      "lola 6.976744186046516\n",
      "arog 6.976744186046516\n",
      "argo 6.976744186046516\n",
      "movie 43 13.953488372093023\n",
      "Top answers from embeddings: avengers endgame, captain america civil war, spiderman homecoming\n",
      "the impossible, spiderman far from home\n",
      "Execution time for answer_query: 6.5731 seconds\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:18.140986Z",
     "start_time": "2024-10-31T13:43:11.407512Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Tell me a good joke\")",
   "id": "3ca2ea020035a86b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: if you want to be a doctor, you have to be a dentist.\n",
      "Execution time for answer_query: 6.7287 seconds\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:22.475394Z",
     "start_time": "2024-10-31T13:43:18.159635Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Hello, how is life?\")",
   "id": "468bb67207d901c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: I'm fine, thanks.\n",
      "Execution time for answer_query: 4.3098 seconds\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:25.497008Z",
     "start_time": "2024-10-31T13:43:22.498290Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Hello, how are you?\")",
   "id": "946f8598f3fde92d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: I'm fine, thanks.\n",
      "Execution time for answer_query: 2.9935 seconds\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:26.813257Z",
     "start_time": "2024-10-31T13:43:25.526676Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Hello, what is the capital of switzerland?\")",
   "id": "31e8c5e0aef44c0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: Bern\n",
      "Execution time for answer_query: 1.2812 seconds\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:30.415141Z",
     "start_time": "2024-10-31T13:43:26.834294Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who is the director of Good Will Hunting?\", \"Gus Van Sant\")",
   "id": "f9402d1107c14c35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: steven soderbergh\n",
      "Execution time for answer_query: 3.5764 seconds\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:33.877796Z",
     "start_time": "2024-10-31T13:43:30.446563Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who directed The Bridge on the River Kwai?\", \"David Lean\")",
   "id": "468416428c921e05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: david lean\n",
      "Execution time for answer_query: 3.4225 seconds\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:37.164750Z",
     "start_time": "2024-10-31T13:43:33.903069Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who directed The Dark Knight?\", \"Christopher Nolan\")",
   "id": "abbc81e04a699a72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: christopher nolan\n",
      "Execution time for answer_query: 3.2563 seconds\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:45.231318Z",
     "start_time": "2024-10-31T13:43:37.192489Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Where was Angelina Jolie born?\", \"Los Angeles\")",
   "id": "b33e8b5221e04a26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER_Person: ['angel']\n",
      "NER_Movies: []\n",
      "Top answers from embeddings: new york city, los angeles, united states of america\n",
      "Angelina Jolie was born in Los Angeles.\n",
      "Execution time for answer_query: 8.0347 seconds\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:47.890635Z",
     "start_time": "2024-10-31T13:43:45.244982Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who is the main actor in harry potter and the philosopher's stone?\")",
   "id": "47ade0f1dd42bdd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: Rupert Grint\n",
      "Execution time for answer_query: 2.6386 seconds\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:50.637842Z",
     "start_time": "2024-10-31T13:43:47.910956Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who was Brad Pitt married to?\")",
   "id": "4a4f8912be5e2679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: jennifer aniston\n",
      "Execution time for answer_query: 2.7212 seconds\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:51.453477Z",
     "start_time": "2024-10-31T13:43:50.666310Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"When was Inception released?\")",
   "id": "b9615c76501defec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: 2010\n",
      "Execution time for answer_query: 0.7807 seconds\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:54.081398Z",
     "start_time": "2024-10-31T13:43:51.479667Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who is the director of Star Wars?\")",
   "id": "42e389076edf07fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: george lucas\n",
      "Execution time for answer_query: 2.5941 seconds\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:55.253771Z",
     "start_time": "2024-10-31T13:43:54.097081Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"When was the Godfather III published?\")",
   "id": "7311213f17afb80c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: 1972\n",
      "Execution time for answer_query: 1.1505 seconds\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:58.259538Z",
     "start_time": "2024-10-31T13:43:55.274680Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who is the director of Star Wars?\")",
   "id": "aadeab7341df5d98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: george lucas\n",
      "Execution time for answer_query: 2.9784 seconds\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:43:59.357599Z",
     "start_time": "2024-10-31T13:43:58.283043Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"When was Inception released?\")",
   "id": "8790e75b87abfd67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: 2010\n",
      "Execution time for answer_query: 1.0705 seconds\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:02.038135Z",
     "start_time": "2024-10-31T13:43:59.376190Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who was Angelina Jolie married to?\")",
   "id": "6659fd0e66bde3ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: bruce willis\n",
      "Execution time for answer_query: 2.6550 seconds\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:04.399354Z",
     "start_time": "2024-10-31T13:44:02.057715Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who was Brad Pitt married to?\")",
   "id": "9e8d79bc11a07fc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: jennifer aniston\n",
      "Execution time for answer_query: 2.3369 seconds\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:06.484491Z",
     "start_time": "2024-10-31T13:44:04.409187Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"What is the religion of Tom Cruise?\")",
   "id": "b5602fe224c6b2c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: agnostic\n",
      "Execution time for answer_query: 2.0710 seconds\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:10.012244Z",
     "start_time": "2024-10-31T13:44:06.510700Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who is the main actor in harry potter and the philosopher's stone?\")",
   "id": "caa8b85465ee7a1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: Rupert Grint\n",
      "Execution time for answer_query: 3.4976 seconds\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:15.745683Z",
     "start_time": "2024-10-31T13:44:10.041950Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who are the cast in Jurassic Park?\")",
   "id": "b8504a68fe32f6ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: johnny depp johnny depp johnny depp\n",
      "Execution time for answer_query: 5.6983 seconds\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:18.431070Z",
     "start_time": "2024-10-31T13:44:15.771928Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who acted in Jurassic Park?\")",
   "id": "caf3e73703fed695",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: michael j fox\n",
      "Execution time for answer_query: 2.6528 seconds\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:21.555331Z",
     "start_time": "2024-10-31T13:44:18.456579Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who played in Jurassic Park?\")",
   "id": "40163a1aa0c34960",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: edward g robinson\n",
      "Execution time for answer_query: 3.0933 seconds\n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:22.842021Z",
     "start_time": "2024-10-31T13:44:21.566100Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"In which movie did Tom Cruise play?\")",
   "id": "14bcf95252a1b280",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: risky business\n",
      "Execution time for answer_query: 1.2705 seconds\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:25.016131Z",
     "start_time": "2024-10-31T13:44:22.865782Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"In which movie did Rebel Wilson act?\")",
   "id": "d65e48ab10da08fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: saturday night fever\n",
      "Execution time for answer_query: 2.1450 seconds\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:44.555338Z",
     "start_time": "2024-10-31T13:44:25.038230Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"In which movie did Liam Neeson play?\")",
   "id": "533e2cd752612e69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER_Person: ['l']\n",
      "NER_Movies: []\n",
      "Top answers from embeddings: larry sanders, larry black, tito ortiz\n",
      "Liam Neeson played in 43.\n",
      "Execution time for answer_query: 19.5011 seconds\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:47.009642Z",
     "start_time": "2024-10-31T13:44:44.567919Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who is an actor in Taken 2?\")",
   "id": "5339c88d922da86d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: ryan reynolds\n",
      "Execution time for answer_query: 2.4334 seconds\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:49.078058Z",
     "start_time": "2024-10-31T13:44:47.016682Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"What is the role of Vin Diesel in Fast and Furious?\")",
   "id": "99208cc1e2af11c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: dolph lundgren\n",
      "Execution time for answer_query: 2.0516 seconds\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:51.292418Z",
     "start_time": "2024-10-31T13:44:49.095494Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"For which movie did Leonardo Di Caprio win an Oscar?\")",
   "id": "2e27a99aeb854ab0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smalltalk: lord of rings\n",
      "Execution time for answer_query: 2.1920 seconds\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:44:51.301974Z",
     "start_time": "2024-10-31T13:44:51.297938Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a2f48cf202f6c12b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
