{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-18T11:18:03.520214Z",
     "start_time": "2024-10-18T11:18:03.516014Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import unicodedata\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import logging as transformers_logging\n",
    "from tabulate import tabulate\n",
    "transformers_logging.set_verbosity_error()\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T11:18:03.539478Z",
     "start_time": "2024-10-18T11:18:03.529004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words_to_keep = [\"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"with\", \"how\", \"before\", \"after\",\"same\"]\n",
    "stop_words = set([s for s in stopwords.words('english') if s not in stop_words_to_keep])"
   ],
   "id": "a2ac0c91ab7407d4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sandr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T11:18:04.110384Z",
     "start_time": "2024-10-18T11:18:03.550855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NERParser:\n",
    "    def __init__(self, model_name: str = \"dslim/bert-base-NER\", lowercase: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the NER parser with a model and optionally configure the lowercase preprocessing.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.lowercase = lowercase\n",
    "        self.device = self.get_device()\n",
    "        \n",
    "        # Load the tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, do_lower_case=self.lowercase)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Set up the NER pipeline\n",
    "        self.nlp_pipeline = pipeline(\n",
    "            \"ner\", \n",
    "            model=self.model, \n",
    "            tokenizer=self.tokenizer, \n",
    "            device=self.device, \n",
    "            aggregation_strategy=\"simple\"\n",
    "        )\n",
    "\n",
    "    def get_device(self):\n",
    "        \"\"\"\n",
    "        Determines whether to use MPS, CUDA, or CPU depending on the available hardware.\n",
    "        \"\"\"\n",
    "        if torch.backends.mps.is_available():\n",
    "            print(\"MPS device found, using MPS backend.\\n\")\n",
    "            return torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            print(f\"CUDA device found, using CUDA backend. Device: {torch.cuda.get_device_name(0)}\\n\")\n",
    "            return torch.device(\"cuda\")\n",
    "        else:\n",
    "            print(\"Neither MPS nor CUDA found, using CPU.\\n\")\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    \n",
    "    def parse_ner_results(self, ner_results: list):\n",
    "        \"\"\"\n",
    "        Parse the NER results and extract entities related to 'PER' (persons) and 'MISC' (potential movie titles).\n",
    "        \"\"\"\n",
    "        per_entities, misc_entities = [], []\n",
    "        \n",
    "        for entity in ner_results:\n",
    "            # Extraction of all Persons\n",
    "            if entity['entity_group'] == 'PER':\n",
    "                per_entities.append(entity['word'])\n",
    "            # Extraction of all Misc that could indicate movies\n",
    "            elif entity['entity_group'] == 'MISC':\n",
    "                misc_entities.append(entity['word'])\n",
    "        \n",
    "        return per_entities, misc_entities\n",
    "\n",
    "    \n",
    "    def process_query(self, query: str):\n",
    "        \"\"\"\n",
    "        Processes a text query, runs NER, and returns the extracted actors and movie names.\n",
    "        \"\"\"\n",
    "        # Optionally lowercase the input if configured\n",
    "        if self.lowercase:\n",
    "            query = query.lower()\n",
    "        \n",
    "        # Run the NER pipeline\n",
    "        ner_results = self.nlp_pipeline(query)\n",
    "\n",
    "        # Parse the results to extract actors and movies\n",
    "        per_entities, misc_entities = self.parse_ner_results(ner_results)\n",
    "        \n",
    "        return per_entities, misc_entities\n",
    "\n",
    "ner_parser = NERParser(lowercase=False)"
   ],
   "id": "d49cf7e85debd577",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither MPS nor CUDA found, using CPU.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T11:18:04.125284Z",
     "start_time": "2024-10-18T11:18:04.118811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataBase:\n",
    "    \"\"\"Handles the extraction of context data for given people and movies from a database, with fuzzy matching for names.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.db = pd.read_pickle(os.path.join(os.getcwd(), r\"exports/extended_graph_triples.pkl\"))\n",
    "        \n",
    "        with open(r\"exports/entity_db.json\", encoding=\"utf-8\") as f: \n",
    "            self.entities = json.load(f)\n",
    "            self.entity_list = list(subject.lower() for subject, types in self.entities.values())\n",
    "        \n",
    "        # otherwise exact matching will fail\n",
    "        self.db['subject_id'] = self.db['subject_id'].astype(str).str.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_string(s):\n",
    "        \"\"\"Cleans the input entity to a uniform naming convention, by removing non ascii characters, encoding it to utf, setting it to lowercase, and removing redundant spaces\"\"\"\n",
    "        s = s.lower()\n",
    "        s = unicodedata.normalize('NFKD', s)\n",
    "        s = s.encode('ascii', 'ignore').decode('utf-8')\n",
    "        s = re.sub(r'[^\\w\\s]', '', s)\n",
    "        s = ' '.join(s.split())\n",
    "        return s\n",
    "    \n",
    "    def fetch(self, entity_lst, search_column):\n",
    "        \n",
    "        relevant = self.db[self.db[search_column].isin(entity_lst)].dropna(axis=1)\n",
    "           \n",
    "        if relevant.empty:\n",
    "            print(f\"No context data found for given information.\")\n",
    "            return pd.DataFrame()          \n",
    "        \n",
    "        pivot_df = relevant.pivot_table(\n",
    "            index='subject_id',\n",
    "            columns='predicate_label',\n",
    "            values='object_label',\n",
    "            aggfunc=lambda x: ' | '.join(x.astype(str))\n",
    "        )\n",
    "    \n",
    "        pivot_df.reset_index(inplace=True)\n",
    "    \n",
    "        return pivot_df"
   ],
   "id": "d17e5503d50ffd8c",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T11:18:04.138121Z",
     "start_time": "2024-10-18T11:18:04.132531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QueryEmbedder:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._glove_embeddings = self._load_glove_embeddings(\"exports/glove.6B/glove.6B.300d.txt\")\n",
    "        \n",
    "    def _load_glove_embeddings(self, file_path):\n",
    "        embeddings = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_phrase(self, phrase):\n",
    "        words = phrase.split()\n",
    "        word_vectors = [self._glove_embeddings[word.lower()] for word in words if word.lower() in self._glove_embeddings]\n",
    "        \n",
    "        if len(word_vectors) == 0:\n",
    "            return np.zeros(300)\n",
    "        \n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    "
   ],
   "id": "ad421631fb9f85b0",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:09:14.825900Z",
     "start_time": "2024-10-18T12:09:14.819003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LLM():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.qa_model = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", top_k=1)\n",
    "    \n",
    "    def query(self, query, context_df, top_columns):\n",
    "        \n",
    "        context = \"\"\n",
    "        for index, row in context_df.iterrows():\n",
    "            node_label = row.get(\"node label\", \"Unknown\")\n",
    "            row_context = f\"This text is about \\\"{node_label}\\\":\\n\"\n",
    "            row_context += \"\\n\".join([f\"{col}: {row[col]}\" for col in context_df[top_columns].columns])\n",
    "            row_context += \"\\n\\n\"\n",
    "            context += row_context\n",
    "        \n",
    "        output = self.qa_model(question=query, context=context)\n",
    "        \n",
    "        answer_str = str()\n",
    "        if isinstance(output, list) and output:\n",
    "            answer_str = \", \".join([result['answer'] for result in output])\n",
    "            \n",
    "        elif isinstance(output, dict):\n",
    "            answer_str = output['answer']\n",
    "        \n",
    "        if not answer_str:\n",
    "            answer_str = \"No answer found.\"\n",
    "        \n",
    "        return answer_str"
   ],
   "id": "e0ce5b9bfb93651f",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:09:16.512119Z",
     "start_time": "2024-10-18T12:09:16.508069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0 \n",
    "    \n",
    "    return np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def find_closest_columns(query_embeddings, column_embeddings, top_n=5):\n",
    "    column_similarities = {}\n",
    "\n",
    "    for col, col_vec in column_embeddings.items():\n",
    "        similarities = [cosine_sim(col_vec, q_vec) for q_vec in query_embeddings if np.linalg.norm(q_vec) > 0]\n",
    "        column_similarities[col] = np.mean(similarities) if similarities else -1\n",
    "\n",
    "    sorted_columns = sorted(column_similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    return [col for col, sim in sorted_columns[:top_n]]"
   ],
   "id": "93858e6040c9c835",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:09:31.506844Z",
     "start_time": "2024-10-18T12:09:17.297004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = DataBase()\n",
    "\n",
    "qe = QueryEmbedder()\n",
    "\n",
    "llm = LLM()"
   ],
   "id": "71ebcb4ff84a1fe4",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T11:18:17.692295Z",
     "start_time": "2024-10-18T11:18:17.685242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fuzzy_match(query_str, comparison_list, threshold=30):\n",
    "    matches = process.extract(query_str, comparison_list, scorer=fuzz.partial_ratio, limit=100)\n",
    "    \n",
    "    id_name_score = []\n",
    "    for match in matches:\n",
    "        name = match[0]\n",
    "        score = match[1]\n",
    "        matched_id = next(key for key, value in db.entities.items() if value[0] == name)\n",
    "            \n",
    "        length_ratio = len(name) / len(query_str)\n",
    "        adjusted_score = score * length_ratio\n",
    "                \n",
    "        id_name_score.append((matched_id, name, adjusted_score))\n",
    "        \n",
    "    # print(tabulate(id_name_score[:3], headers=[\"id\", \"Name\", \"Score\"], tablefmt=\"grid\"))\n",
    "    \n",
    "    return [id for id, _, score in id_name_score if score >= threshold]"
   ],
   "id": "b7975593c212a460",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:09:33.106492Z",
     "start_time": "2024-10-18T12:09:33.100960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_matches(df, normalized_query, top_n=2):\n",
    "    concatenated_rows = df.apply(lambda row: ' '.join(row.astype(str)), axis=1).tolist()\n",
    "    top_matches = process.extract(normalized_query, concatenated_rows, scorer=fuzz.partial_ratio, limit=top_n)\n",
    "    \n",
    "    # Extract the indices of the top matches\n",
    "    top_indices = [match[2] for match in top_matches]\n",
    "    \n",
    "    # Return the rows corresponding to the top matches\n",
    "    return df.iloc[top_indices]"
   ],
   "id": "68ede21d279d32fa",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:23:40.839294Z",
     "start_time": "2024-10-18T12:23:40.822556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def answer_query(query, correct_answer=\"\"):\n",
    "    normalized_query = db.normalize_string(query)\n",
    "    \n",
    "    index_matches = fuzzy_match(normalized_query, db.entity_list, threshold=30)\n",
    "    \n",
    "    context = db.fetch(index_matches, \"subject_id\")\n",
    "    \n",
    "    # EXPERIMENTAL\n",
    "    context = get_top_matches(context, normalized_query, top_n=1)\n",
    "    \n",
    "    if context.empty:\n",
    "        print(\"No context data found for given IDs or string\")\n",
    "        context = pd.DataFrame()\n",
    "        \n",
    "    column_embeddings = {col: qe.embed_phrase(col) for col in context.columns}\n",
    "    query_embeddings = [qe.embed_phrase(word) for word in query.split()]  \n",
    "    top_columns_embeddings = find_closest_columns(query_embeddings, column_embeddings, top_n=2)\n",
    "    \n",
    "    # EXPERIMENTAL\n",
    "    top_columns_dict = process.extract(normalized_query, context.columns, scorer=fuzz.partial_ratio, limit=2)\n",
    "    top_columns_fuzzy = [c[0] for c in top_columns_dict]\n",
    "    top_columns_embeddings = []\n",
    "    \n",
    "    top_columns = list(set(top_columns_fuzzy + top_columns_embeddings + [\"node label\"]))\n",
    "    \n",
    "    filtered_context_df = context[top_columns]\n",
    "        \n",
    "    answer = llm.query(query, filtered_context_df, top_columns)\n",
    "    \n",
    "    ### EXPERIMENTAL\n",
    "    answer_df = pd.DataFrame([answer], columns=[\"answer\"])\n",
    "    answer = llm.query(query, answer_df, [\"answer\"])\n",
    "    \n",
    "    normalized_answer = db.normalize_string(answer)\n",
    "    normalized_correct_answer = db.normalize_string(correct_answer)\n",
    "    \n",
    "    print(f\"{'CORRECT' if normalized_answer == normalized_correct_answer else 'WRONG'} - {query} - {answer}\")\n"
   ],
   "id": "f64a89b6c62d95e4",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:23:42.874897Z",
     "start_time": "2024-10-18T12:23:42.055560Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who is the director of Good Will Hunting?\", \"Gus Van Sant\")",
   "id": "b92daea468f8ae0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT - Who is the director of Good Will Hunting? - gus van sant\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:23:44.955638Z",
     "start_time": "2024-10-18T12:23:44.251088Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who directed The Bridge on the River Kwai?\", \"David Lean\")",
   "id": "fc09a07aff4c27e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRONG - Who directed The Bridge on the River Kwai? - david\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:23:48.535460Z",
     "start_time": "2024-10-18T12:23:47.880466Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who directed the movie The Godfather?\", \"Francis Ford Coppola\") # or Mario Puzo or Francis Ford Coppola",
   "id": "a6056ee8bb4f64ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT - Who directed the movie The Godfather? - francis ford coppola\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:23:50.713527Z",
     "start_time": "2024-10-18T12:23:50.039676Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who is the director of The Dark Knight?\", \"Christopher Nolan\")",
   "id": "8d9752b180ab30d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT - Who is the director of The Dark Knight? - christopher nolan\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:23:52.379973Z",
     "start_time": "2024-10-18T12:23:51.740414Z"
    }
   },
   "cell_type": "code",
   "source": "answer_query(\"Who directed The Dark Knight?\", \"Christopher Nolan\")",
   "id": "73e2d76165df3d84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT - Who directed The Dark Knight? - christopher nolan\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "14626d0f4be528dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
