{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T13:11:26.458301Z",
     "start_time": "2024-11-19T13:11:26.132663Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:14:17.028400Z",
     "start_time": "2024-11-19T13:14:17.020662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "factual = pd.read_csv(\"factual_questions_500.csv\")\n",
    "recommendation = pd.read_csv(\"movie_recommendation_questions_500.csv\")\n",
    "multimedia = pd.read_csv(\"multimedia_questions_500.csv\")\n",
    "unrelated = pd.read_csv(\"unrelated_queries_500.csv\")"
   ],
   "id": "9c43fa213b82a86d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:14:34.112111Z",
     "start_time": "2024-11-19T13:14:34.106159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "factual['Label'] = 0          # 0 for factual questions\n",
    "recommendation['Label'] = 1   # 1 for recommendation questions\n",
    "multimedia['Label'] = 2       # 2 for multimedia questions\n",
    "unrelated['Label'] = 3        # 3 for unrelated questions\n",
    "\n",
    "combined_data = pd.concat([factual, recommendation, multimedia, unrelated], ignore_index=True)\n",
    "\n",
    "combined_data = combined_data.sample(frac=1).reset_index(drop=True)"
   ],
   "id": "972cd3d2789509ab",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:27:21.682594Z",
     "start_time": "2024-11-19T13:27:21.677141Z"
    }
   },
   "cell_type": "code",
   "source": "print(combined_data)",
   "id": "fc97f5f7fae89433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Question  Label\n",
      "0                              When was Alien released?      0\n",
      "1              Can I see what Sofia Vergara looks like?      2\n",
      "2                         When was Goodfellas released?      0\n",
      "3              What are some films with Robert De Niro?      1\n",
      "4                            How tall is Mount Everest?      3\n",
      "...                                                 ...    ...\n",
      "1995            Can you show me a photo of Johnny Depp?      2\n",
      "1996         I'd like to see a picture of Kristen Bell.      2\n",
      "1997  Looking for movies directed by Paul Thomas And...      1\n",
      "1998    Recommend musicals like La La Land and Chicago.      1\n",
      "1999                  What genre is A Clockwork Orange?      0\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:18:32.817941Z",
     "start_time": "2024-11-19T13:16:54.264604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          Trainer, TrainingArguments)\n",
    "\n",
    "factual = pd.read_csv(\"factual_questions_500.csv\")\n",
    "recommendation = pd.read_csv(\"movie_recommendation_questions_500.csv\")\n",
    "multimedia = pd.read_csv(\"multimedia_questions_500.csv\")\n",
    "unrelated = pd.read_csv(\"unrelated_queries_500.csv\")\n",
    "\n",
    "factual['Label'] = 0          # 0 for factual questions\n",
    "recommendation['Label'] = 1   # 1 for recommendation questions\n",
    "multimedia['Label'] = 2       # 2 for multimedia questions\n",
    "unrelated['Label'] = 3        # 3 for unrelated questions\n",
    "\n",
    "# Combine and shuffle the data\n",
    "combined_data = pd.concat([factual, recommendation, multimedia, unrelated], ignore_index=True)\n",
    "combined_data = combined_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = combined_data['Question'].values\n",
    "y = combined_data['Label'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "train_dataset = QuestionDataset(\n",
    "    texts=X_train,\n",
    "    labels=y_train,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "val_dataset = QuestionDataset(\n",
    "    texts=X_val,\n",
    "    labels=y_val,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=16,   \n",
    "    warmup_steps=50,                 \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    "    compute_metrics=compute_metrics      \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_result}\")\n",
    "\n",
    "trainer.save_model('question_classifier_model')\n",
    "tokenizer.save_pretrained('question_classifier_model')\n"
   ],
   "id": "e95ccd528eca51ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86c4a523d4ab49fcbfb458a4444f9d24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "949d064dac9a4c6cb706560ed0cfda27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61fd49988a3b494f9bcdc49a219ee6f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6998320595e4a879420564ba3447b0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ee2aa8413894ce9ae6c04ad577d4e92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/kevinbrundler/Desktop/ATAI/movie-bot/.venv/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [339/339 00:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.244490</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.979968</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.006449833977967501, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 0.5552, 'eval_samples_per_second': 360.262, 'eval_steps_per_second': 23.417, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('question_classifier_model/tokenizer_config.json',\n",
       " 'question_classifier_model/special_tokens_map.json',\n",
       " 'question_classifier_model/vocab.txt',\n",
       " 'question_classifier_model/added_tokens.json',\n",
       " 'question_classifier_model/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:39:59.430548Z",
     "start_time": "2024-11-19T13:39:59.425981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class QueryRouter:\n",
    "    def __init__(self, model_path=r'./question_classifier_model'):\n",
    "        \"\"\"\n",
    "        Initializes the QuestionClassifier with a pre-trained model and tokenizer.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the directory where the fine-tuned model and tokenizer are saved.\n",
    "        \"\"\"\n",
    "        # Load the tokenizer and model from the specified directory\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "        # Mapping from label indices to question types\n",
    "        self.label_map = {\n",
    "            0: 'factual',\n",
    "            1: 'recommendation',\n",
    "            2: 'multimedia',\n",
    "            3: 'unrelated'\n",
    "        }\n",
    "\n",
    "    def predict(self, query):\n",
    "        \"\"\"\n",
    "        Classifies a single question into one of the predefined categories.\n",
    "\n",
    "        Args:\n",
    "            question (str): The input question to classify.\n",
    "\n",
    "        Returns:\n",
    "            str: The predicted category label as a string.\n",
    "        \"\"\"\n",
    "        # Tokenization and Encoding of Query\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            query,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        # Prediction\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "        return self.label_map[predicted_class]"
   ],
   "id": "b16b92905f5100c9",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:39:59.968848Z",
     "start_time": "2024-11-19T13:39:59.934031Z"
    }
   },
   "cell_type": "code",
   "source": "qr = QueryRouter()",
   "id": "f41dab97e0c6f2ed",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:35:44.608804Z",
     "start_time": "2024-11-19T13:35:44.565618Z"
    }
   },
   "cell_type": "code",
   "source": "qr.predict(\"hello how are you?\")",
   "id": "6c88482409f6c3ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unrelated'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a2f897dc85d1f25e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
