{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-20T08:25:16.654046Z",
     "start_time": "2024-11-20T08:25:16.648577Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T08:25:17.379313Z",
     "start_time": "2024-11-20T08:25:17.367608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "factual = pd.read_csv(\"factual_questions_500.csv\")\n",
    "recommendation = pd.read_csv(\"movie_recommendation_questions_500.csv\")\n",
    "multimedia = pd.read_csv(\"multimedia_questions_500.csv\")\n",
    "unrelated = pd.read_csv(\"unrelated_queries_500.csv\")"
   ],
   "id": "9c43fa213b82a86d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T08:25:18.419899Z",
     "start_time": "2024-11-20T08:25:18.411641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "factual['Label'] = 0          # 0 for factual questions\n",
    "recommendation['Label'] = 1   # 1 for recommendation questions\n",
    "multimedia['Label'] = 2       # 2 for multimedia questions\n",
    "unrelated['Label'] = 3        # 3 for unrelated questions\n",
    "\n",
    "combined_data = pd.concat([factual, recommendation, multimedia, unrelated], ignore_index=True)\n",
    "\n",
    "combined_data = combined_data.sample(frac=1).reset_index(drop=True)"
   ],
   "id": "972cd3d2789509ab",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T08:25:19.278061Z",
     "start_time": "2024-11-20T08:25:19.275218Z"
    }
   },
   "cell_type": "code",
   "source": "print(combined_data)",
   "id": "fc97f5f7fae89433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Question  Label\n",
      "0                  Show me a photo of Chris Hemsworth.      2\n",
      "1                    Any good films with Meryl Streep?      1\n",
      "2      Any historical thrillers with espionage themes?      1\n",
      "3           What is the runtime of Lawrence of Arabia?      0\n",
      "4     Recommend movies directed by Guillermo del Toro.      1\n",
      "...                                                ...    ...\n",
      "2303          I'd like to see a picture of Will Smith.      2\n",
      "2304                    Any musicals set in the 1920s?      1\n",
      "2305      Any animated films with classic fairy tales?      1\n",
      "2306                 What does Marisa Tomei look like?      2\n",
      "2307            What movies are similar to Casablanca?      1\n",
      "\n",
      "[2308 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T08:26:34.833763Z",
     "start_time": "2024-11-20T08:26:12.315736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          Trainer, TrainingArguments)\n",
    "\n",
    "factual = pd.read_csv(\"factual_questions_500.csv\")\n",
    "recommendation = pd.read_csv(\"movie_recommendation_questions_500.csv\")\n",
    "multimedia = pd.read_csv(\"multimedia_questions_500.csv\")\n",
    "unrelated = pd.read_csv(\"unrelated_queries_500.csv\")\n",
    "\n",
    "factual['Label'] = 0          # 0 for factual questions\n",
    "recommendation['Label'] = 1   # 1 for recommendation questions\n",
    "multimedia['Label'] = 2       # 2 for multimedia questions\n",
    "unrelated['Label'] = 3        # 3 for unrelated questions\n",
    "\n",
    "# Combine and shuffle the data\n",
    "combined_data = pd.concat([factual, recommendation, multimedia, unrelated], ignore_index=True)\n",
    "combined_data = combined_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = combined_data['Question'].values\n",
    "y = combined_data['Label'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "train_dataset = QuestionDataset(\n",
    "    texts=X_train,\n",
    "    labels=y_train,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "val_dataset = QuestionDataset(\n",
    "    texts=X_val,\n",
    "    labels=y_val,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=1,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=16,   \n",
    "    warmup_steps=50,                 \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    "    compute_metrics=compute_metrics      \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_result}\")\n",
    "\n",
    "trainer.save_model('question_classifier_model')\n",
    "tokenizer.save_pretrained('question_classifier_model')\n"
   ],
   "id": "e95ccd528eca51ba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/kevinbrundler/Desktop/ATAI/movie-bot/.venv/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 00:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.470300</td>\n",
       "      <td>0.266831</td>\n",
       "      <td>0.956710</td>\n",
       "      <td>0.956273</td>\n",
       "      <td>0.963925</td>\n",
       "      <td>0.956710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.011836</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.011835739947855473, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 0.6542, 'eval_samples_per_second': 353.079, 'eval_steps_per_second': 22.927, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('question_classifier_model/tokenizer_config.json',\n",
       " 'question_classifier_model/special_tokens_map.json',\n",
       " 'question_classifier_model/vocab.txt',\n",
       " 'question_classifier_model/added_tokens.json',\n",
       " 'question_classifier_model/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:39:59.430548Z",
     "start_time": "2024-11-19T13:39:59.425981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class QueryRouter:\n",
    "    def __init__(self, model_path=r'./question_classifier_model'):\n",
    "        \"\"\"\n",
    "        Initializes the QuestionClassifier with a pre-trained model and tokenizer.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the directory where the fine-tuned model and tokenizer are saved.\n",
    "        \"\"\"\n",
    "        # Load the tokenizer and model from the specified directory\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "        # Mapping from label indices to question types\n",
    "        self.label_map = {\n",
    "            0: 'factual',\n",
    "            1: 'recommendation',\n",
    "            2: 'multimedia',\n",
    "            3: 'unrelated'\n",
    "        }\n",
    "\n",
    "    def predict(self, query):\n",
    "        \"\"\"\n",
    "        Classifies a single question into one of the predefined categories.\n",
    "\n",
    "        Args:\n",
    "            question (str): The input question to classify.\n",
    "\n",
    "        Returns:\n",
    "            str: The predicted category label as a string.\n",
    "        \"\"\"\n",
    "        # Tokenization and Encoding of Query\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            query,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        # Prediction\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "        return self.label_map[predicted_class]"
   ],
   "id": "b16b92905f5100c9",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:39:59.968848Z",
     "start_time": "2024-11-19T13:39:59.934031Z"
    }
   },
   "cell_type": "code",
   "source": "qr = QueryRouter()",
   "id": "f41dab97e0c6f2ed",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:35:44.608804Z",
     "start_time": "2024-11-19T13:35:44.565618Z"
    }
   },
   "cell_type": "code",
   "source": "qr.predict(\"hello how are you?\")",
   "id": "6c88482409f6c3ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unrelated'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a2f897dc85d1f25e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
